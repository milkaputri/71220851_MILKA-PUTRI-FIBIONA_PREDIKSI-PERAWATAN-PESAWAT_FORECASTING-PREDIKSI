{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3vaj5mRGrRm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **UJIAN AKHIR SEMESTER - SEMESTER GENAP 2023/2024**\n",
        "#### **KECERDASAN BUATAN**\n",
        "---------------------------------------------------------------------------\n",
        "NAMA : MILKA PUTRI FIBIONA\n",
        "#### NIM  : 71220851\n"
      ],
      "metadata": {
        "id": "yhC_5E-ErUhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '[Dataset]_Test_(Perawatan-Pesawat).csv'\n",
        "data = pd.read_csv('[Dataset]_Test_(Perawatan-Pesawat).csv')\n",
        "\n",
        "# Pengecekan Data Kotor\n",
        "def check_data_quality(df):\n",
        "    # Memeriksa nilai yang hilang\n",
        "    missing_values = df.isnull().sum()\n",
        "\n",
        "    # Memeriksa nilai duplikat\n",
        "    duplicate_values = df.duplicated().sum()\n",
        "\n",
        "    # Memeriksa nilai yang tidak valid (misalnya nilai yang sangat kecil atau sangat besar)\n",
        "    outlier_values = {}\n",
        "    for column in df.select_dtypes(include=[np.number]).columns:\n",
        "        outliers = df[(df[column] < df[column].quantile(0.01)) | (df[column] > df[column].quantile(0.99))]\n",
        "        outlier_values[column] = len(outliers)\n",
        "\n",
        "    return missing_values, duplicate_values, outlier_values\n",
        "\n",
        "# Memeriksa kualitas data\n",
        "missing_values, duplicate_values, outlier_values = check_data_quality(data)\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "print(\"\\nDuplicate Values:\\n\", duplicate_values)\n",
        "print(\"\\nOutlier Values:\\n\", outlier_values)\n",
        "\n",
        "# Pembersihan Data\n",
        "def clean_data(df):\n",
        "    # Mengisi nilai yang hilang dengan median atau cara lain yang sesuai\n",
        "    df = df.fillna(df.median())\n",
        "\n",
        "    # Menghapus nilai duplikat\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Mengatasi nilai yang tidak valid dengan mengganti outlier menggunakan IQR\n",
        "    for column in df.select_dtypes(include=[np.number]).columns:\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
        "        df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Membersihkan data\n",
        "cleaned_data = clean_data(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCXBUfKsXVb",
        "outputId": "36733895-d2ac-4a3d-d23f-56c91086c6a8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values:\n",
            " id           0\n",
            "cycle        0\n",
            "setting1     0\n",
            "setting2     0\n",
            "setting3     0\n",
            "            ..\n",
            "sd20         0\n",
            "sd21         0\n",
            "ttf          0\n",
            "label_bnc    0\n",
            "label_mcc    0\n",
            "Length: 71, dtype: int64\n",
            "\n",
            "Duplicate Values:\n",
            " 0\n",
            "\n",
            "Outlier Values:\n",
            " {'id': 2, 'cycle': 2, 'setting1': 2, 'setting2': 0, 'setting3': 0, 's1': 0, 's2': 2, 's3': 2, 's4': 2, 's5': 0, 's6': 1, 's7': 2, 's8': 2, 's9': 2, 's10': 0, 's11': 1, 's12': 2, 's13': 2, 's14': 2, 's15': 2, 's16': 0, 's17': 1, 's18': 0, 's19': 0, 's20': 2, 's21': 2, 'av1': 0, 'av2': 2, 'av3': 2, 'av4': 2, 'av5': 0, 'av6': 0, 'av7': 2, 'av8': 2, 'av9': 2, 'av10': 0, 'av11': 2, 'av12': 2, 'av13': 2, 'av14': 2, 'av15': 2, 'av16': 0, 'av17': 2, 'av18': 0, 'av19': 0, 'av20': 2, 'av21': 2, 'sd1': 0, 'sd2': 2, 'sd3': 2, 'sd4': 2, 'sd5': 0, 'sd6': 1, 'sd7': 2, 'sd8': 2, 'sd9': 2, 'sd10': 0, 'sd11': 2, 'sd12': 2, 'sd13': 2, 'sd14': 2, 'sd15': 2, 'sd16': 0, 'sd17': 1, 'sd18': 0, 'sd19': 0, 'sd20': 2, 'sd21': 2, 'ttf': 2, 'label_bnc': 0, 'label_mcc': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTtc0DIZxK3u",
        "outputId": "a36adc61-2761-4f3c-e2e5-c0b718cc8853"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 71 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   id         100 non-null    int64  \n",
            " 1   cycle      100 non-null    int64  \n",
            " 2   setting1   100 non-null    float64\n",
            " 3   setting2   100 non-null    float64\n",
            " 4   setting3   100 non-null    float64\n",
            " 5   s1         100 non-null    float64\n",
            " 6   s2         100 non-null    float64\n",
            " 7   s3         100 non-null    float64\n",
            " 8   s4         100 non-null    float64\n",
            " 9   s5         100 non-null    float64\n",
            " 10  s6         100 non-null    float64\n",
            " 11  s7         100 non-null    float64\n",
            " 12  s8         100 non-null    float64\n",
            " 13  s9         100 non-null    float64\n",
            " 14  s10        100 non-null    float64\n",
            " 15  s11        100 non-null    float64\n",
            " 16  s12        100 non-null    float64\n",
            " 17  s13        100 non-null    float64\n",
            " 18  s14        100 non-null    float64\n",
            " 19  s15        100 non-null    float64\n",
            " 20  s16        100 non-null    float64\n",
            " 21  s17        100 non-null    int64  \n",
            " 22  s18        100 non-null    int64  \n",
            " 23  s19        100 non-null    float64\n",
            " 24  s20        100 non-null    float64\n",
            " 25  s21        100 non-null    float64\n",
            " 26  av1        100 non-null    float64\n",
            " 27  av2        100 non-null    float64\n",
            " 28  av3        100 non-null    float64\n",
            " 29  av4        100 non-null    float64\n",
            " 30  av5        100 non-null    float64\n",
            " 31  av6        100 non-null    float64\n",
            " 32  av7        100 non-null    float64\n",
            " 33  av8        100 non-null    float64\n",
            " 34  av9        100 non-null    float64\n",
            " 35  av10       100 non-null    float64\n",
            " 36  av11       100 non-null    float64\n",
            " 37  av12       100 non-null    float64\n",
            " 38  av13       100 non-null    float64\n",
            " 39  av14       100 non-null    float64\n",
            " 40  av15       100 non-null    float64\n",
            " 41  av16       100 non-null    float64\n",
            " 42  av17       100 non-null    float64\n",
            " 43  av18       100 non-null    float64\n",
            " 44  av19       100 non-null    float64\n",
            " 45  av20       100 non-null    float64\n",
            " 46  av21       100 non-null    float64\n",
            " 47  sd1        100 non-null    float64\n",
            " 48  sd2        100 non-null    float64\n",
            " 49  sd3        100 non-null    float64\n",
            " 50  sd4        100 non-null    float64\n",
            " 51  sd5        100 non-null    float64\n",
            " 52  sd6        100 non-null    float64\n",
            " 53  sd7        100 non-null    float64\n",
            " 54  sd8        100 non-null    float64\n",
            " 55  sd9        100 non-null    float64\n",
            " 56  sd10       100 non-null    float64\n",
            " 57  sd11       100 non-null    float64\n",
            " 58  sd12       100 non-null    float64\n",
            " 59  sd13       100 non-null    float64\n",
            " 60  sd14       100 non-null    float64\n",
            " 61  sd15       100 non-null    float64\n",
            " 62  sd16       100 non-null    float64\n",
            " 63  sd17       100 non-null    float64\n",
            " 64  sd18       100 non-null    float64\n",
            " 65  sd19       100 non-null    float64\n",
            " 66  sd20       100 non-null    float64\n",
            " 67  sd21       100 non-null    float64\n",
            " 68  ttf        100 non-null    int64  \n",
            " 69  label_bnc  100 non-null    int64  \n",
            " 70  label_mcc  100 non-null    int64  \n",
            "dtypes: float64(64), int64(7)\n",
            "memory usage: 55.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNJLDxWDx8rQ",
        "outputId": "95efd15b-5379-46c5-9587-2035837aaf8b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi Data\n",
        "def normalize_data(df):\n",
        "    # Drop irrelevant columns for normalization\n",
        "    columns_to_normalize = df.drop(columns=['id', 'cycle', 'label_bnc', 'label_mcc', 'ttf']).columns\n",
        "\n",
        "    # Initialize the MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Normalisasi data yang telah dibersihkan\n",
        "normalized_data = normalize_data(cleaned_data)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "normalized_data = normalized_data.drop(columns=['id', 'cycle', 'label_bnc', 'label_mcc'])\n",
        "\n",
        "# Memastikan hanya kolom yang relevan yang digunakan untuk prediksi\n",
        "X = normalized_data.drop(columns=['ttf'])\n",
        "y = normalized_data['ttf']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Membuat Model Random Forest Regressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Evaluasi Model\n",
        "train_mse = mean_squared_error(y_train, y_pred_train)\n",
        "test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "train_r2 = r2_score(y_train, y_pred_train)\n",
        "test_r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Train MSE: {train_mse}, Test MSE: {test_mse}\")\n",
        "print(f\"Train R2: {train_r2}, Test R2: {test_r2}\")\n",
        "\n",
        "# Fungsi untuk Mengonversi Prediksi ttf ke Persentase Kerusakan\n",
        "def predict_damage_percentage(features, model, max_ttf=200):\n",
        "    ttf_pred = model.predict(features)\n",
        "    ttf_pred = np.clip(ttf_pred, 0, max_ttf)  # Pastikan nilai prediksi tidak melebihi max_ttf\n",
        "    damage_percentage = 100 * (max_ttf - ttf_pred) / max_ttf\n",
        "    return damage_percentage\n",
        "\n",
        "# Contoh penggunaan dengan satu sampel dari X_test\n",
        "sample_features = X_test.iloc[0].values.reshape(1, -1)\n",
        "damage_percentage = predict_damage_percentage(sample_features, model)\n",
        "print(f\"Predicted Damage Percentage: {damage_percentage[0]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KtZoGA0yU-K",
        "outputId": "8eddef3f-8639-4f0e-cd70-2f3f06fe0915"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSE: 75.71281875, Test MSE: 381.33827999999994\n",
            "Train R2: 0.9568274241995096, Test R2: 0.7617718943417915\n",
            "Predicted Damage Percentage: 71.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi Model dengan MAE\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Train MAE: {train_mae}, Test MAE: {test_mae}\")\n",
        "\n",
        "# Evaluasi Model dengan R-squared\n",
        "train_r2 = r2_score(y_train, y_pred_train)\n",
        "test_r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Train R2: {train_r2}, Test R2: {test_r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jakNEWgWltyB",
        "outputId": "3b09ae41-54ed-45ae-a254-26a5bd7a9b1e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MAE: 6.2731249999999985, Test MAE: 13.798000000000002\n",
            "Train R2: 0.9568274241995096, Test R2: 0.7617718943417915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Presentasi akurasi\n",
        "# Evaluasi Model dengan MAE\n",
        "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Train MAE: {train_mae}, Test MAE: {test_mae}\")\n",
        "\n",
        "# Akurasi prediksi persentase kerusakan\n",
        "accuracy = 100 - test_mae  # Karena persentase kerusakan dihitung dari 100 - MAE\n",
        "print(f\"Accuracy of Damage Percentage Prediction: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl9WBck0xFRB",
        "outputId": "4af3a32b-a7bf-4e5b-f919-7a9ba24148e8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MAE: 6.2731249999999985, Test MAE: 13.798000000000002\n",
            "Accuracy of Damage Percentage Prediction: 86.20%\n"
          ]
        }
      ]
    }
  ]
}